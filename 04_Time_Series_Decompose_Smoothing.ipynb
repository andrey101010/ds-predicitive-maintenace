{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBlvS0NTbI5w"
   },
   "source": [
    "# Time Series Simulation Study\n",
    "In this notebook you make a simulation study to investigate the class of ARMA models. You will see that this class of models is \n",
    "- easily applicable with the `statsmodel` package,\n",
    "- can be used for simulation of time series data,\n",
    "- showing good forecasting abilities.\n",
    "\n",
    "At the end of this notebook you will have learned:\n",
    "- how to simulate time series data,\n",
    "- how to define, estimate and forecast with ARMA and SARIMA models,\n",
    "- how to decompose a time series into a trend, seasonality and rest,\n",
    "- how to define Exponential Smoothing estimators and apply them to the data for forecasting,\n",
    "- how to apply stationarty tests and\n",
    "- how to apply ACF, PACF and Periodogram plots for model diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txheazGI6Pgv"
   },
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1884,
     "status": "ok",
     "timestamp": 1616609925004,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "4LYuHrSDbLbc",
    "outputId": "9dfb8c30-477f-4c6d-867b-f75e1f2de6f1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy import signal\n",
    "%matplotlib inline\n",
    "\n",
    "# If you want a style choose one\n",
    "#plt.style.use('Solarize_Light2')\n",
    "#plt.style.use('tableau-colorblind10')\n",
    "NF_ORANGE = '#ff5a36'\n",
    "NF_BLUE = '#163251'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93a9xEln1B_d"
   },
   "source": [
    "See all matplotlib styles under [matplotlib styles](https://problemsolvingwithpython.com/06-Plotting-with-Matplotlib/06.13-Plot-Styles/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBQERR036TA_"
   },
   "source": [
    "## Define auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1616609930612,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "Qpi4KSSFAJF4"
   },
   "source": [
    "The following functions are defined to simplify time series plotting, time differencing and stationarity testing. We will use them through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1616609932493,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "eViMpDPwhCJN"
   },
   "outputs": [],
   "source": [
    "def plot_ts(ts = None, ts_add = None, title ='Time Series', legend=['1']):\n",
    "    \"\"\"\n",
    "    Plots one or two time series in a single plot\n",
    "    \n",
    "        Args:\n",
    "        ts: 1d- or 2d-array of time series. Dimension\n",
    "            must be (n,) or (n,2)\n",
    "        title: Title for the time plot.\n",
    "        legend: list of legend names. If empty no legend.\n",
    "        \n",
    "        Returns:\n",
    "        matplotlib plot object\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(ts[:,], color=NF_ORANGE)\n",
    "    plt.grid(True,axis='y')\n",
    "    plt.title(title)\n",
    "    if ts_add is not None:\n",
    "        plt.plot(ts_add, color=NF_BLUE)\n",
    "    if len(legend) > 0:\n",
    "        plt.legend(legend)\n",
    "    plt.show()\n",
    "\n",
    "def plot_acf_pacf(ts, lags=10, layout='h'):\n",
    "    \"\"\"\n",
    "    Plots the empirical ACF and PACF of a time series process.\n",
    "    \n",
    "        Args:\n",
    "        ts: array of time series\n",
    "        \n",
    "        Returns:\n",
    "        matplotlib subplot with ACF and PACF\n",
    "    \"\"\"\n",
    "    if layout == 'h':\n",
    "        fig, ax = plt.subplots(1, 2, figsize = (10,3))\n",
    "    else: \n",
    "        fig, ax = plt.subplots(2, 1, figsize = (10,3))\n",
    "    sm.tsa.graphics.plot_acf(ts,color = NF_ORANGE,lags=lags, ax = ax[0])\n",
    "    sm.tsa.graphics.plot_pacf(ts,color = NF_ORANGE, lags = lags, ax = ax[1])    \n",
    "\n",
    "\n",
    "def diff_series(ts, interval=1):\n",
    "    \"\"\"\n",
    "    Differences a time series by a certain lag.\n",
    "    \n",
    "        Args:\n",
    "        ts: array of 1d time series\n",
    "        \n",
    "        Returns:\n",
    "        Differenced time series\n",
    "    \"\"\"\n",
    "    diff = ts[interval:] - ts[:-interval]\n",
    "    return diff\n",
    "\n",
    "def kpss_test(ts):\n",
    "    \"\"\"\n",
    "    Performs a KPSS test for the null hypothesis of stationarity.\n",
    "    \n",
    "        Args:\n",
    "        ts: 1d time series\n",
    "        \n",
    "        Returns:\n",
    "        Summary of test statistic and critical values\n",
    "    \"\"\"\n",
    "    print ('Results of KPSS Test:')\n",
    "    kpsstest = kpss(ts, regression='c', nlags='legacy')\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print (kpss_output)\n",
    "\n",
    "def adf_test(ts):\n",
    "    \"\"\"\n",
    "    Performs a Dickey-Fuller test for the null hypothesis of\n",
    "    non-stationarity.\n",
    "    \n",
    "        Args:\n",
    "        ts: 1-d time series\n",
    "    \n",
    "        Returns:\n",
    "        Printed test statistic and critical values.\n",
    "    \"\"\"\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(ts, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used',\n",
    "                                             'Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimates all resemble closely the true parameters in the DGP above and show high significance. In this case we have some complex roots (the only real one is greater than 1 and therefore we do not have a unit root). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend and seasonality\n",
    "-------------------\n",
    "As already mentioned in lecture, trends and seasonalities make a time series process non-stationary as they induce time-dependent distributions, i.e. values of $X_t$ occurring between $t$ and $t+h$ are differently distributed than values between times $t+k$ and $t+k+h$. In such cases the standard theory cannot be applied as stationary processes are assumed in this theory. \n",
    "\n",
    "It therefore needs a detrending and removing of seasonal components. In the following we consider (1) the composition of time series by a trend a seasonal component and a time-independent dynamic process. For this reason we sample data from a normal distribution (our white noise) and define two trend functions \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_t&=\\alpha_1+\\alpha_2\\cdot t\\\\\n",
    "m_t&=\\beta_1+\\beta_2\\cdot t + \\beta_3\\cdot t^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "that we add to the white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-PMKdRNhyns"
   },
   "outputs": [],
   "source": [
    "# Define a DGP with a linear and quadratic trend\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample a normal white noise \n",
    "y = np.random.normal(0.0, 0.4, 100)\n",
    "\n",
    "# Construct the time series with linear and quadratic trend\n",
    "trend = 0.1 + 0.07 * np.arange(0,100)\n",
    "y_ln = trend + y\n",
    "y_qd = y_ln + 0.001 * np.arange(0,100)**2\n",
    "\n",
    "# Plot the series\n",
    "plot_ts(y_ln, y_qd, title = 'Trends', legend=['Linear', 'Qaudratic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNlUfaYKGUTD"
   },
   "source": [
    "### Estimate a quadratic trend\n",
    "You learned that there are two methods to eliminate the trend in a time series. The first was estimating it and eliminate this estimate from the original time series. The other method is time differencing. In what follows we use the first method to eliminate the trend from the time series. \n",
    "\n",
    "We define a white noise process with 300 observations and compose a time series process by a qaudratic trend component with parameters $\\alpha_1=0.2$, $\\alpha_2=0.01$ and $\\alpha_3=0.001$ and let the innovations be white noise normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EniIksbQyD-"
   },
   "outputs": [],
   "source": [
    "# Define the data generating process\n",
    "np.random.seed(42)\n",
    "error = np.random.normal(0.0, 1.1, 300)\n",
    "trend = 0.2 + 0.01 * np.arange(1, 301) + 0.001 * np.arange(1, 301)**2\n",
    "y = trend + error\n",
    "\n",
    "# Plot the time series\n",
    "plot_ts(y,title='', legend=['Quadratic Trend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we estimate this trend? The most obvious approach is to use OLS with polynomial features in $t$:\n",
    "\n",
    "$\n",
    "x_t=\\alpha_1+\\alpha_2 t + \\alpha_3 t^2\n",
    "$\n",
    "\n",
    "to estimate the coefficients $\\alpha_i~,i=1,2,3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1606470047740,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "WZ3zqsJXTHGp",
    "outputId": "b61720a9-19a7-495e-ee89-4a31f6d4e46d"
   },
   "outputs": [],
   "source": [
    "# Estimate a linear regression model \n",
    "\n",
    "# Start by constructing a time index variable and its square\n",
    "X = np.array([np.arange(1, 301),np.arange(1, 301)**2]).T\n",
    "\n",
    "# Use a constant term in the OLS regression\n",
    "X = sm.tools.tools.add_constant(X)\n",
    "model = sm.OLS(y, X)\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the constant term is highly non-significant and with a farout estimate. However, the coefficients for the time index and its square are near the true values of $[0.01, 0.001]$. To see, if the predictions are good enough we plot the time series together with the estimated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1606470073061,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "sqdyUgiGTiRF",
    "outputId": "ea9479bd-9ac2-4a72-8beb-9bb07d55c452"
   },
   "outputs": [],
   "source": [
    "# Plot the original and estimated time series\n",
    "y_est = res.predict(X)\n",
    "plot_ts(y, y_est, title='', legend=['Orig.', 'Est.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That appears to be a good estimate of the quadratic trend immanent in the time series. \n",
    "\n",
    ">__Exercise__: Take the original time series and difference the estimated one. Plot your results. What do you see? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P82j1KYnczQr"
   },
   "source": [
    "### Seasonality\n",
    "Next to trends, seasonality has an influence on the distribution(s) of a time series, as seasonality implies that at certain times the process explores other value ranges than at others. This again would be a non-stationary setting and would make the standard theory for time series modeling to fail. \n",
    "\n",
    "By eliminating seasonalities (any cyclical pattern) from the time series data such a non-stationary time process can be transformed to a stationary one. Similar to trend elmination also seasonalities can be removed by two main approaches: (1) modeling the cyclical pattern and removing an estimate of this model from the original time series or (2) differencing the original time series at periodical time indices (e.g. a quarterly cyclical pattern has been finished after 4 time steps if data is quarterly),\n",
    "\n",
    "$\n",
    "\\Delta(4)x_t=x_t-x_{t-4}~.\n",
    "$\n",
    "\n",
    "In the following we model the cyclical pattern by a _Fourier Series_ with a single sinus function and period 60 (e.g. 60 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1616587594718,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "hGnZB4bDc0Xl",
    "outputId": "c81990bb-0536-4340-e7d7-27553ddfebdf"
   },
   "outputs": [],
   "source": [
    "# Define the DGP\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the white noise process\n",
    "error = np.random.normal(0.0, 1.1, 300)\n",
    "\n",
    "# Define the seasonal pattern\n",
    "# Use Fourier Series with a single sinus pattern\n",
    "# with period 60.\n",
    "seasonal = np.sin((2*np.pi/60)*np.arange(1,301))\n",
    "\n",
    "# Combine the two terms and plot the series\n",
    "y = seasonal + error\n",
    "plot_ts(y, title = 'Seasonality',legend=['Cycle d=60'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cyclical pattern in the time series can be seen clearly. It is also easy to detect that the period of the cyclical pattern is 60 as after 60 time steps the process has arrived at a similar level as where it was before 60 steps. \n",
    "\n",
    ">__Exercise__: Model a cyclical pattern by an OLS equation using sinusoidal function of the time. Estimate this model and plot it together with the original time series process seen above. If the estimate is good enough in your eyes, difference this estimate from the original time series and plot the remaining term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuSmxhOJapon"
   },
   "source": [
    "#### Trend and seasonal component\n",
    "What we usually observe or encounter in time series is a mixture of both, i.e. some trend and some cyclical pattern (sometimes not even easily detectable). In the following we generate a time series with such properties and you will decompose the model into its different components, i.e. (1) trend, (2) seasonal, and (3) residuals. Residuals might show no white noise properties (e.g. autocorrelation) and that would then need to be modeled as well (e.g. with an ARMA model). \n",
    "\n",
    "In the following you create a time series with a normally distributed innovation vector together with a sinuoidal seasonal pattern and a linear trend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1616587598263,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "7OKwdS18duME",
    "outputId": "4ff8aea3-63ec-424e-db75-752fc7a62daa"
   },
   "outputs": [],
   "source": [
    "# Generate the data with trend and seasonal component\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the white noise process\n",
    "error = np.random.normal(0.0, 1.1, 300)\n",
    "\n",
    "# Define the seasonal term\n",
    "seasonal = np.sin((2*np.pi/60)*np.arange(1,301))\n",
    "\n",
    "# Define the trend component\n",
    "trend = 0.01 + 0.05 * np.arange(1, 301) \n",
    "\n",
    "# Add all components together and plot them\n",
    "y = trend + seasonal + error\n",
    "plot_ts(y, title='Time series with trend and seasonality', legend=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already like time series we might observe in the news or in business. As the decomposition is part of time series EDA, there exist pre-defined functions to decompose time series into their three components. In the `statsmodels` module the function `tsa.seasonal_decompose()` can be used to make such a decomposition. For the cyclical component you have to define a period length. (Take also a look at the multiplicative version of it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 1966,
     "status": "ok",
     "timestamp": 1616587601177,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "kj2qam49u9v8",
    "outputId": "01fb1f72-9e15-4e5a-d1a5-768c38b6b11e"
   },
   "outputs": [],
   "source": [
    "# Decompose the time series into trend, seasonal component and residuals\n",
    "y_dec = sm.tsa.seasonal_decompose(y, model = 'additive', period=60)\n",
    "y_dec.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals show white noise behavior. And to see, if residuals have really no autocorrelations, we could do some diagnostic checks and tests. \n",
    "\n",
    ">__Exercise__: For the residuals process from the decomposition in `y_dec`. Plot the ACF and PACF. Take also a look at the [_Ljung-Box-Test_](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html) and apply this test to your residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH22lz0NsDB4"
   },
   "source": [
    "#### Example with a composed model\n",
    "\n",
    "Remember: any stationary zero-mean time series can be expressed as a harmonic sum of (infinitely many) sinus and cosinus waves (very seldom this sum is finite - therefore modeling with such a harmonic mean a whole time series is avoided). The periodicity $d$ of a seasonal trend then determines the rate at which a cycle is finished in one time step:\n",
    "\n",
    "$\\frac{2\\pi}{d}t$\n",
    "\n",
    "We can also say $\\frac{1}{d}$ is the frequency of the cycle, i.e. if $d=4$ a quarter of the whole cycle is concluded at a single time step. Trigonometrically, the cycle concludes $0.25\\cdot2\\pi$ _radiants_ per time step and a whole cycle after 4 time steps.\n",
    "\n",
    "In the following simulation we model the seasonal component as a Fourier Series with a single sinus function and a periodicity of 4. A linear trend is added ogether with an AR(2) process. Note that in this case the white noise is generated inside of the `generate_sample()` method. The innvoation term $\\varepsilon$ therewith is white noise, however, the residuum of a time series decomposition will be not, as this process is then an AR(2) process such that residuals from the time decomposiion would be autocorrelated (the Ljung-Box test can be applied here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1616625083560,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "JnWRpq_gLK5F",
    "outputId": "840529dd-db8f-4709-9c10-c17b1f7f6666"
   },
   "outputs": [],
   "source": [
    "# Generate an AR(2) process\n",
    "np.random.seed(42)\n",
    "ar_proc = sm.tsa.ArmaProcess(ar = [1, -.7, -.2]).generate_sample(nsample=1000)\n",
    "\n",
    "# Define trend and seasonal component\n",
    "seasonal = 1.3 * np.sin(2*np.pi/4*np.arange(1,1001))\n",
    "trend = 0.01 + 0.02 * np.arange(1, 1001) \n",
    "y = trend + ar_proc + seasonal\n",
    "plot_ts(y,legend=[],title='Composed time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the cycle is somehow still apparent, even though it got washed out a little by the AR(2) component. \n",
    "\n",
    "In a real-world time series approach the first step after looking at the graph above, would be a trend elimination. This time we use the second method for trend removal, i.e. the time differencing approach: \n",
    "\n",
    "$\n",
    "\\Delta x_t=x_t - x_{t-1}~.\n",
    "$\n",
    "\n",
    "Time differencing works well, if the trend is linear. With a non-linear trend we usually have to difference multiple times to arrive at a stationary process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1616625085651,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "ZWYsWydggRM4",
    "outputId": "0eb2a2b3-cc67-4e05-eaf1-b3758faa2175"
   },
   "outputs": [],
   "source": [
    "# Detrend the series\n",
    "y_diff = diff_series(y)\n",
    "plot_ts(y_diff, title = '', legend=['1st Diff.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1616625087078,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "GUfmgioIi2gg",
    "outputId": "c0b0c679-da41-49c4-8f6c-29171d1450e9"
   },
   "outputs": [],
   "source": [
    "# We use for the sampling frequency 1 as we want to\n",
    "# discover cycles over the original time steps.\n",
    "f, Pxx = signal.periodogram(y_diff, fs = 1, window='hamming', scaling='spectrum')\n",
    "plt.plot(f, Pxx, color = NF_ORANGE)\n",
    "plt.title('Periodogram')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominant frequency here identified by the periodogram lays at around $\\frac{1}{d}=0.25$ which corresponds to a periodicity of $d=4$, exactly the periodicity of the simulated seasonality. We can also observe that there exist many more cyclical patterns with other frequencies (observe the little trembling along the line), however the one at frequency 0.25 is the by far most dominant one. By using the periodogram we can identify what frequencies are included in our time series and which are the most dominant ones. By eliminating the most dominant ones we arrive at some point at a stationary time series that allows us explicit modeling via an ARMA model for example. \n",
    "\n",
    "As the periodicity in our time series appears to be at 4 (identified by the periodogram), we clean our series from this cyclical pattern by taking a difference between the actual value and its lag four time periods before:\n",
    "\n",
    "$\n",
    "\\Delta(4) x_t=x_t-x_{t-4}~.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1616625088592,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "fBmpm4srqokJ",
    "outputId": "218be683-4c07-459a-b63d-82e3253ee1a6"
   },
   "outputs": [],
   "source": [
    "# Difference the time series and plot\n",
    "y_diff_4 = diff_series(y_diff, 4)\n",
    "plot_ts(y_diff_4[:200,], title='', legend=['Diff(1,4)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph it appears that the dominant cycle which was clearly visual has been removed from the series. If this is the case should be checked by a periodogram on the cleaned time series and also a stationarity test. We look at such stationarity tests in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-IW2GP8QaYz"
   },
   "source": [
    "#### Testing for unit roots (non-stationarity)\n",
    "After differencing to eliminate trend and seasonal components, we want to check, if we were able to arrive at a stationary time series. There exist some main procedures for testing on stationarity, namely [Philipps Perron](https://en.wikipedia.org/wiki/Phillips%E2%80%93Perron_test#cite_note-2) test, [augmented Dickey-Fuller (ADF)](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test) test, and the [Kwiatkowski-Phillips-Schmidt-Shin (KPSS)](https://en.wikipedia.org/wiki/KPSS_test). \n",
    "\n",
    "In contrast to the Phillips-Perron and augmented Dickey-Fuller the KPSS test the other way around, i.e. it tests on (trend-)stationarity, where trend-stationarity is the property of a time series to be stationary after elimination of a linear trend (by e.g. first-differencing it). The other two tests test the null of non-stationarity (unit-root).\n",
    "\n",
    "Here we only apply the augmented Dickey-Fuller and KPSS tests to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1616625091110,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "Bh-LzBT3y74c",
    "outputId": "8048120f-738e-4550-b8df-4f05d853c68e"
   },
   "outputs": [],
   "source": [
    "# Performa an augmented Dickey-Fuller test on non-stationarity.\n",
    "adf_test(y_diff_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM45Q1FkQzOn"
   },
   "source": [
    "As the $p$-value is clearly below 1% we can reject the null hypothesis of non-stationarity. \n",
    "\n",
    "To be sure we make also an KPSS test that tests the other way around: a null hypothesis of (trend-)stationarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1616625093668,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "itnae5J3y_fR",
    "outputId": "8fc2b37a-4249-431c-d9e5-b1547f2e34f6"
   },
   "outputs": [],
   "source": [
    "# Perform KPSS test on stationarity\n",
    "kpss_test(y_diff_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLgjobb3RDbZ"
   },
   "source": [
    "The p-value is 10% and we cannot reject the null hypothesis of stationarity for our differenced time series. \n",
    "\n",
    "In regard to the tests we have with high certainty a time series that is indeed stationary. With stationary time series we can now proceed our time series analysis and estimate an ARMA model. \n",
    "\n",
    ">__Exercise__: Apply also the Phillips-Perron test and write a corresponding test function as in `kpss_test()` or `adf_test()` to report results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN4umvyJEl_f"
   },
   "source": [
    "## Exponential Smoothing\n",
    "As mentioned in lecture a very strong benchmark class of time series models is the class of exponential smoothing models, where the actual values are modeled as exponential averages between the last value and past values:\n",
    "\n",
    "$\n",
    "\\hat{x}_{t+1}=\\alpha x_t + (1-\\alpha)\\hat{x}_{t-1}~.\n",
    "$\n",
    "\n",
    "These exponential smoothers are simple, but hard to beat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_varZkWBK3VI"
   },
   "source": [
    "### Import the exponential smoothing functions\n",
    "We will use a very simple exponential smoothing method and then the well-known [Holt-Winters](https://otexts.com/fpp2/holt-winters.html) smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1616628516734,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "k6UHkayj97Rq"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxJR9ez_LGi2"
   },
   "source": [
    "### Simple exponential smoothing without seasonality and trend\n",
    "We use in the following the time series process simulated above for the SARIMA model. The simple exponential smoothing method takes only an average over past values and does assume no trend or seasonality in the data. We should expect that it does not perform well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1616628518913,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "uEj2GFagC_1R",
    "outputId": "f06373d9-dd74-4bf8-f0c2-4e2684c4ec57"
   },
   "outputs": [],
   "source": [
    "# Use a smoothing level alpha of 0.4, i.e.\n",
    "# past values will be given a weight of 60%\n",
    "# actual values a weight of the remaining 40% \n",
    "simple_exp = SimpleExpSmoothing(y, initialization_method='estimated').fit(smoothing_level=0.6,optimized=False)\n",
    "\n",
    "# Predict the last 20 values in the original\n",
    "# time series\n",
    "pred = simple_exp.predict(980, 999)\n",
    "y_pred = np.ndarray((100,))\n",
    "\n",
    "# Set the points not forecasted to NaN \n",
    "# (this does not plot them)\n",
    "y_pred[:80] = np.NaN \n",
    "y_pred[80:] = pred\n",
    "\n",
    "# Print the forecast RMSE.\n",
    "print('In-sample RMSE: {}'.format(np.sqrt(np.sum((y[980:] - pred)**2))))\n",
    "plot_ts(y[900:], y_pred, title = '', legend=['Orig.', 'Est.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vb1BdIJjLfuD"
   },
   "source": [
    "As expected the simple exponential smoother that does not take into account the linear trend apparent in the simulated time series data does bad. We have an RMSE of around 7.5 (whereas the RMSE for the SARIMA model was at 4.2. We see that the exponential smoother exhibits some forerunning, which is caused by neglecting two string dynamics in the simulated time series. \n",
    "\n",
    "### Holt-Winters exponential smoothing with trend and seasonality\n",
    "Holt-Winters is a smoothing approach that is usually very hard to beat in practice. Let us see what the smoother can achieve on this simulated data. In contrast to the simple exponential smoother, the Holt-Winters method accounts for a trend and a seasonal component in the data. We can choose either an _additive_ or _multiplicative_ approach. As we do know that the trend and seasonal component have been added and not multiplicated to the white noise we choose the additive method `add` for these two components of the Holt-Winters smoother. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1616628522403,
     "user": {
      "displayName": "Simon Zehnder",
      "photoUrl": "",
      "userId": "10763557501609397435"
     },
     "user_tz": -60
    },
    "id": "MRMet_KrFeEE",
    "outputId": "17bdd773-40a6-4f9a-ab48-303eb38eb40c"
   },
   "outputs": [],
   "source": [
    "# Use an additive seasonality and additive trend. \n",
    "# As we know from prior EDA, we have a cycle of 4. \n",
    "holt_winters = ExponentialSmoothing(y, seasonal_periods=4,seasonal='add', trend='add', initialization_method='estimated')\n",
    "res = holt_winters.fit()\n",
    "\n",
    "# Make a prediction for the last 20 values. \n",
    "pred = res.predict(980, 999)\n",
    "y_pred = np.ndarray((100,))\n",
    "\n",
    "# Set the points not forecasted to NaN \n",
    "# (this does not plot them)\n",
    "y_pred[:80] = np.NaN \n",
    "y_pred[80:] = pred\n",
    "\n",
    "# Print the forecast RMSE.\n",
    "print('In-sample RMSE: {}'.format(np.sqrt(np.sum((y[980:1000] - pred)**2))))\n",
    "plot_ts(y[900:], y_pred, title = '', legend=['Orig.', 'Est.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhYawcrZMBPe"
   },
   "source": [
    "As expected the Holt-Winters exponential smoothing performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO4Dv1YHpbpXRiA3cP0J6Qg",
   "collapsed_sections": [],
   "name": "nf_time_series_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
